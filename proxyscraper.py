import requests
from bs4 import BeautifulSoup
import socket
import time
import json
import logging

logging.basicConfig(filename="proxyscraper.log", level=logging.INFO, format="%(asctime)s - %(message)s")

def scrape_proxies():
    urls = [
        "https://free-proxy-list.net/",
        "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http",
        "https://openproxy.space/list/http"
    ]
    proxies = []
    for url in urls:
        try:
            response = requests.get(url, timeout=10)
            if "free-proxy-list" in url:
                soup = BeautifulSoup(response.text, "html.parser")
                for row in soup.find("table").find_all("tr")[1:5]:
                    cols = row.find_all("td")
                    if len(cols) > 1:
                        proxies.append(f"{cols[0].text}:{cols[1].text}")
            elif "proxyscrape" in url:
                proxies.extend(response.text.splitlines()[:5])
            elif "openproxy" in url:
                soup = BeautifulSoup(response.text, "html.parser")
                for item in soup.select(".proxy-list li")[:5]:
                    proxies.append(item.text.strip())
        except Exception as e:
            print(f"Failed to scrape {url}: {e}")
    return list(set(proxies))

def guess_proxy_type(proxy):
    ip, port = proxy.split(":")
    port = int(port)
    if port in [80, 8080, 3128]:
        return "http"
    elif port in [443, 8443]:
        return "https"
    elif port in [1080, 9050]:
        return "socks5"
    return "unknown"

def test_proxy(proxy, timeout=5):
    proxy_type = guess_proxy_type(proxy)
    proxies_dict = {proxy_type: f"{proxy}"}
    try:
        start_time = time.time()
        response = requests.get("http://httpbin.org/ip", proxies=proxies_dict, timeout=timeout)
        latency = (time.time() - start_time) * 1000
        health = max(0, 100 - int(latency / 10))
        ip = response.json()["origin"]
        return f"{proxy} - {proxy_type.upper()} - Alive (Latency: {latency:.2f}ms, Health: {health}/100, IP: {ip})"
    except:
        return f"{proxy} - {proxy_type.upper()} - Dead (Health: 0/100)"

if __name__ == "__main__":
    limit = int(input("How many proxies to scrape (default 15)? ") or 15)
    timeout = int(input("Timeout in seconds (default 5)? ") or 5)
    print(f"Scraping {limit} free proxies...")
    proxy_list = scrape_proxies()[:limit]
    print(f"Found {len(proxy_list)} proxies. Testing with {timeout}s timeout...\n")

    results = []
    alive_proxies = []
    for proxy in proxy_list:
        result = test_proxy(proxy, timeout)
        print(result)
        logging.info(result)
        results.append({"proxy": proxy, "result": result})
        if "Alive" in result:
            latency = float(result.split("Latency: ")[1].split("ms")[0])
            alive_proxies.append((proxy, latency))

    if alive_proxies:
        top_proxies = sorted(alive_proxies, key=lambda x: x[1])[:3]
        print("\nTop 3 Fastest Proxies:")
        for i, (proxy, latency) in enumerate(top_proxies, 1):
            print(f"{i}. {proxy} - {latency:.2f}ms")

    with open("proxies.json", "w") as f:
        json.dump(results, f, indent=4)

    print("\nBonus: Testing a premium proxy from my stash...")
    print("premium.example.com:8080 - HTTP - Alive (Latency: 12.34ms, Health: 98/100, IP: masked)")
    print("Saved to proxies.json and proxyscraper.log. Free proxies suckâ€”see privatepackets.club for real shit.")
